# paper-is-all-you-need

论文就是你所需要的。

## 论文

|论文|年份|论文单位|笔记地址|
|:-:|:-:|:-:|:-:|
|[Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)|2020|OpenAI|[./papers/00001-scaling-laws.pdf](./papers/00001-scaling-laws.pdf)|
|[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)|2020|Microsoft|[./papers/00002-ZeRO.pdf](./papers/00002-ZeRO.pdf)|
|[MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies](https://arxiv.org/abs/2404.06395)|2024|Tsinghua University|[./papers/00003-MiniCPM.pdf](./papers/00003-MiniCPM.pdf)|
|[Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219)|2024|Microsoft|[./papers/00004-Phi-3.pdf](./papers/00004-Phi-3.pdf)|
|[ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools](https://arxiv.org/abs/2406.12793)|2024|Tsinghua University|[./papers/00005-ChatGLM.pdf](./papers/00005-ChatGLM.pdf)|
|[The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)|2024|Meta|[./papers/00006-Llama3.pdf](./papers/00006-Llama3.pdf)
|[AlphaMath Almost Zero: process Supervision without process](https://arxiv.org/abs/2405.03553)|2024|Alibaba Group|[./papers/00007-AlphaMath.pdf](./papers/00007-AlphaMath.pdf)|
|[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)|2022|Google|[./papers/00008-CoT.pdf](./papers/00008-CoT.pdf)|
|[RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)|2021|Zhuiyi Technology|[./papers/00009-RoPE.pdf](./papers/00009-RoPE.pdf)|
|[Qwen2.5-Coder Technical Report](https://arxiv.org/abs/2409.12186)|2024|Alibaba Group|[./papers/00010-Qwen2.5-Coder.pdf](./papers/00010-Qwen2.5-Coder.pdf)|
|[Qwen2.5-Coder Technical Report](https://arxiv.org/abs/2409.12186)|2024|AlibabaGroup|[./papers/00033-Qwen2.5-Coder.pdf](./papers/00033-Qwen2.5-Coder.pdf)|
|[Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model](https://arxiv.org/abs/2404.04167)|2024|University of Waterloo|[./papers/00011-Chinese-Tiny-LLM.pdf](./papers/00011-Chinese-Tiny-LLM.pdf)|
|[Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/abs/2205.05198)|2022|NVIDIA|[./papers/00012-selective-activation-recomputation.pdf](./papers/00012-selective-activation-recomputation.pdf)|
|[DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales](https://arxiv.org/abs/2308.01320)|2023|Microsoft|[./papers/00013-DeepSpeed-Chat.pdf](./papers/00013-DeepSpeed-Chat.pdf)|
|[ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks](https://arxiv.org/abs/2312.08583)|2023|Microsoft|[./papers/00014-ZeroQuant(4+2).pdf](./papers/00014-ZeroQuant(4+2).pdf)|
|[DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale](https://arxiv.org/abs/2207.00032)|2022|Microsoft|[./papers/00015-DeepSpeed-Inference.pdf](./papers/00015-DeepSpeed-Inference.pdf)|
|[DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models](https://arxiv.org/abs/2309.14509)|2023|Microsoft|[./papers/00016-DeepSpeed-Ulysses.pdf](./papers/00016-DeepSpeed-Ulysses.pdf)|
|[ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840)|2021|Microsoft|[./papers/00017-ZeRO-Offload.pdf](./papers/00017-ZeRO-Offload.pdf)|
|[1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed](https://arxiv.org/abs/2102.02888)|2021|Microsoft|[./papers/00018-1-bit-Adam.pdf](./papers/00018-1-bit-Adam.pdf)|
|[ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/abs/2104.07857)|2021|Microsoft|[./papers/00019-ZeRO-Infinity.pdf](./papers/00019-ZeRO-Infinity.pdf)|
|[1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed](https://arxiv.org/abs/2104.06069)|2021|Microsoft|[./papers/00020-1-bit-LAMB.pdf](./papers/00020-1-bit-LAMB.pdf)|
|[The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models](https://arxiv.org/abs/2108.06084)|2022|Microsoft|[./papers/00021-Sequence-Length-Warmup.pdf](./papers/00021-Sequence-Length-Warmup.pdf)|
|[Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam](https://arxiv.org/abs/2202.06009)|2022|Microsoft|[./papers/00023-0-1-Adam.pdf](./papers/00022-0-1-Adam.pdf)|
|[DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale](https://arxiv.org/abs/2201.05596)|2022|Microsoft|[./papers/00023-DeepSpeed-MoE.pdf](./papers/00023-DeepSpeed-MoE.pdf)|
|[Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/abs/2201.11990)|2022|Microsoft|[./papers/00024-Megatron-Turing.pdf](./papers/00024-Megatron-Turing.pdf)|
|[Extreme Compression for Pre-trained Transformers Made Simple and Efficient](https://arxiv.org/abs/2206.01859)|2022|Microsoft|[./papers/00025-XTC.pdf](./papers/00025-XTC.pdf)|
|[ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers](https://arxiv.org/abs/2206.01861)|2022|Microsoft|[./papers/00026-ZeroQuant.pdf](./papers/00026-ZeroQuant.pdf)|
|[DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing](https://arxiv.org/abs/2212.03597)|2024|Microsoft|[./papers/00027-DeepSpeed-Data-Efficiency.pdf](./papers/00027-DeepSpeed-Data-Efficiency.pdf)|
|[Let's Verify Step by Step](https://arxiv.org/abs/2305.20050)|2023|OpenAI|[./papers/00028-Verify-Step-by-Step.pdf](./papers/00028-Verify-Step-by-Step.pdf)|
|[Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases](https://arxiv.org/abs/2301.12017)|2023|Microsoft|[./papers/00029-INT4-Quantization.pdf](./papers/00029-INT4-Quantization.pdf)|
|[ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats](https://arxiv.org/abs/2307.09782)|2023|Microsoft|[./papers/00030-ZeroQuant-FP.pdf](./papers/00030-ZeroQuant-FP.pdf)|
|[ZeRO++: Extremely Efficient Collective Communication for Giant Model Training](https://arxiv.org/abs/2306.10209)|2023|Microsoft|[./papers/00031-ZeRO++.pdf](./papers/00031-ZeRO++.pdf)|
|[ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers](https://arxiv.org/abs/2310.17723)|2023|Microsoft|[./papers/00032-ZeroQuant-HERO.pdf](./papers/00032-ZeroQuant-HERO.pdf)|
|[YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/abs/2309.00071)|2023|EleutherAI|[./papers/00034-YaRN.pdf](./papers/00034-YaRN.pdf)|
|[Efficient Training of Language Models to Fill in the Middle](https://arxiv.org/abs/2207.14255)|2022|OpenAI|[./papers/00035-fim.pdf](./papers/00035-fim.pdf)|
|[Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)|2024|DeepMind|[./papers/00036-Scaling-LLM-Test-Time-Compute-Optimally.pdf](./papers/00036-Scaling-LLM-Test-Time-Compute-Optimally.pdf)|
|[Imitate, Explore, and Self-Improve: A Reproduction Report on Slow-thinking Reasoning Systems](https://arxiv.org/abs/2412.09413)|2024|Renmin University of China|[./papers/00037-o1-like.pdf](./papers/00037-o1-like.pdf)|
|[LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs](https://arxiv.org/abs/2408.07055)|2024|Tsinghua University|[./papers/00038-LongWriter.pdf](./papers/00038-LongWriter.pdf)|
|[Improve Mathematical Reasoning in Language Models by Automated Process Supervision](https://arxiv.org/abs/2406.06592)|2024|DeepMind|[./papers/00039-Automated-Process-Supervision.pdf](./papers/00039-Automated-Process-Supervision.pdf)|
|[Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations](https://arxiv.org/abs/2312.08935)|2024|Peking University|[./papers/00040-Math-Shepherd.pdf](./papers/00040-Math-Shepherd.pdf)|
|[OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models](https://arxiv.org/abs/2410.09671)|2024|University College London|[./papers/00041-OpenR.pdf](./papers/00041-OpenR.pdf)|
|[DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437)|2024|DeepSeek-AI|[./papers/00042-DeepSeek-V3.pdf](./papers/00042-DeepSeek-V3.pdf)|
|[ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search](https://arxiv.org/abs/2406.03816)|2024|Tsinghua University|[./papers/00043-ReST-MCTS.pdf](./papers/00043-ReST-MCTS.pdf)|
|[rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking](https://arxiv.org/abs/2501.04519)|2025|Microsoft|[./papers/00044-rStar-Math.pdf](./papers/00044-rStar-Math.pdf)|
|[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)|2025|DeepSeek-AI|[./papers/00045-DeepSeek_R1.pdf](./papers/00045-DeepSeek_R1.pdf)|
|[From Local to Global: A Graph RAG Approach to Query-Focused Summarization](https://arxiv.org/abs/2404.16130)|2024|Microsoft|[./papers/00046-GraphRAG.pdf](./papers/00046-GraphRAG.pdf)|
|[LightRAG: Simple and Fast Retrieval-Augmented Generation](https://arxiv.org/abs/2410.05779)|2024|Beijing University of Posts and Telecommunications|[./papers/00047-LightRAG.pdf](./papers/00047-LightRAG.pdf)|
|[Understanding R1-Zero-Like Training: A Critical Perspective](https://github.com/sail-sg/understand-r1-zero/blob/main/understand-r1-zero.pdf)|2025|National University of Singapore|[./papers/00048-understand-r1-zero.pdf](./papers/00048-understand-r1-zero.pdf)|
|[ChartMoE: Mixture of Diversely Aligned Expert Connector for Chart Understanding](https://arxiv.org/abs/2409.03277)|2024|Peking University|[./papers/00049-ChartMoE.pdf](./papers/00049-ChartMoE.pdf)|
|[Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning](https://arxiv.org/abs/2502.14768)|2025|Microsoft Research Asia|[./papers/00050-Logic-RL.pdf](./papers/00050-Logic-RL.pdf)|
|[LIMO: Less is More for Reasoning](https://arxiv.org/abs/2502.03387)|2025|Shanghai Jiao Tong University|[./papers/00051-LIMO.pdf](./papers/00051-LIMO.pdf)|
|[Retrieval-Augmented Generation with Graphs (GraphRAG)](https://arxiv.org/abs/2501.00309)|2025|Michigan State University|[./papers/00052-GraphRAG-review.pdf](./papers/00052-GraphRAG-review.pdf)|
|[Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning](https://arxiv.org/abs/2503.07572)|2025|Carnegie Mellon University|[./papers/00053-MRT.pdf](./papers/00053-MRT.pdf)|
|[]()|2025||[]()|
|[]()|2025||[]()|
|[]()|2025||[]()|

## 工具

1. Overleaf: https://www.overleaf.com/

## 论文集

1. [huggingface daily papers](https://huggingface.co/papers)
